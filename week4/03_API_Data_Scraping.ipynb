{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Big Data And Society: Lab 3\n",
    "=====\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIs and Data Scraping: Getting Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Set up a Twitter Application\n",
    "\n",
    "This week we are going to scrape data from the Twitter API and make some plots! We are going to use the Twitter REST API, which lets us query and retreive samples of Tweets. To do this, you need API keys that are associated with your account. Your API keys are secret and unique to you and only you, and they gives you access to Twitter data through the API.\n",
    "\n",
    "There are a couple of ways to get Twitter data, the REST API is just one of them. The others are to set up a Streamer (which streams real time tweets), or to access the Firehose (this means everything!). Read [this article](https://brightplanet.com/2013/06/twitter-firehose-vs-twitter-api-whats-the-difference-and-why-should-you-care/) for a nice comparison between the methods.\n",
    "\n",
    "The Twitter REST API is best place to start and what we will use in class. Follow the following steps to get your keys.\n",
    "\n",
    "* Create a twitter account if you do not already have one.\n",
    "* Go to [https://apps.twitter.com/](https://apps.twitter.com/) and log in with your twitter credentials.\n",
    "* Click **\"Create New App\"** in the upper right corner\n",
    "* Fill out the form, give it a name like **data_getter_yourname** and a description, agree to the terms, and click **\"Create your Twitter application\"** *Note: The form will ask for a website. You can put the github.io page you made as the URL. **It is OK for your callback URL to be blank.***\n",
    "* In the next page, click on **Keys and Access Tokens** tab, and copy your \"Consumer Key (API Key)\" and \"Consumer Key (API Secret)\".\n",
    "\n",
    "We need a Python file that will contain the **Twitter** keys. It is never a good idea to host these keys on a public website like **github**, so one way to keep them private is importing the keys as a variable from a separate, untracked file. Open your text editor, and in the materials for the week, *PASTE* these keys into the **twitter-keys.py** file and save the file. Using this method, we can then import the keys and use them on a repeated basis, and we can choose not to put this file on Github. **Make sure they are saved as strings!**\n",
    "\n",
    "**!!IMPORTANT NOTE!! NEVER SAVE YOUR TWITTER_KEY.PY FILE TO GITHUB. IF YOU DO, PEOPLE CAN SEE YOUR KEYS AND USE THEM. KEEP THESE ON YOUR MACHINE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing the Libraries and Twitter Keys\n",
    "\n",
    "We will be using **Twython**, a Python library that provides wrappers around Twitter's API. To install **Twython** on a terminal or the command line, run the following command:\n",
    "\n",
    "```\n",
    "pip install twython\n",
    "```\n",
    "\n",
    "We also need to import the keys we just got from Twitter to be accessible to our code. View the contents of **twitter_key.py** to see your keys. These should both be strings.\n",
    "\n",
    "```\n",
    "# In the file you should define two variables:\n",
    "api_key = \"your twitter key\"\n",
    "api_secret = \"your twitter secret\"\n",
    "```\n",
    "\n",
    "Import the libraries:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from twython import Twython\n",
    "\n",
    "# Imports the keys from the python file\n",
    "from twitter_key import api_key, api_secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set Up Twython, Get an OAuth2 Token, and Create your Twython Object\n",
    "\n",
    "In this next step, assign the keys to variables and set up our instance of Twython to work with your account.\n",
    "\n",
    "Doing this requires our authentication with Twitter using our keys. Twitter uses something called [OAuth](https://dev.twitter.com/oauth) for API authentication. There are two types of OAuth authentication. OAuth1 provides user authentication to the API, and is required to post tweets and issue requests on behalf of users. OAuth 2 is [application-only authentication](https://dev.twitter.com/oauth/application-only), and has higher rate limits but doesn't allow you to post on users behalf. Because we can get more tweets using it, we are going to use OAuth2. \n",
    "\n",
    "**OAuth 2 requires a Third Access token you must request using the API. This next step will set everything up for us.**\n",
    "\n",
    "In the last part of this, we create a **Twython** object and call it **my_twython**; this object simplifies the access to the [Twitter API](https://dev.twitter.com/overview/documentation), and provides methods for accessing the APIâ€™s endpoints. The first function fetches tweets with a given query at a given lat-long. We will be using the search parameters to hit the APIs endpoint. We need to provide the lat/lon of the centroid of the area we want to query, maximum number of tweets to return, and area within the centroid to search for, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assigns the keys to the variables\n",
    "APP_KEY = api_key\n",
    "APP_SECRET = api_secret\n",
    "\n",
    "# Create a Twython object called Twitter\n",
    "# Set this up using your Twitter Keys\n",
    "# Say we are going to use OAuth 2\n",
    "twython_setup = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
    "\n",
    "# Get an OAuth2 access token, save as variable so we can launch our \n",
    "OAUTH2_ACCESS_TOKEN = twython_setup.obtain_access_token()\n",
    "\n",
    "# Create a Twython Object we will use for our access to the API\n",
    "my_twython = Twython(APP_KEY, access_token=OAUTH2_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, the Twython documentation and all available commands can be found here. [https://twython.readthedocs.io/en/latest/usage/starting_out.html](https://twython.readthedocs.io/en/latest/usage/starting_out.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Query the Twitter API to get Tweets at a Location\n",
    "\n",
    "Next, let's do a search and get some tweets! Specifically, set up a function that will use the **search** API. Read more about the **search** API [here](https://dev.twitter.com/rest/reference/get/search/tweets).\n",
    "\n",
    "The **Search API** can take many parameters for querying tweets. Twitter has a nice page of what you can use as query parameters here [https://dev.twitter.com/rest/public/search](https://dev.twitter.com/rest/public/search).\n",
    "\n",
    "The Twitter API has rate limits that limit how quickly you can download data. This is to try to lighten the load on their servers. We are using the Search API with **OAuth2 (Application) Access** - which limits us to **450 in 15 minutes** Read about the [Rate Limits](https://dev.twitter.com/rest/public/rate-limiting) here. This means, in our following steps, always follow the guideline that you will not be able to get more than 450 tweets in 15 minutes, or Twitter might lock your access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input the search term you want to search on\n",
    "search_term='' # SET A SEARCH TERM LIKE 'TRUMP'\n",
    "# CAN LEAVE search_term BLANK IF YOU WANT ALL TWEETS NEAR A SPECIFIC LOCATION\n",
    "# Setup a Lat Lon\n",
    "latlong=[40.261455,-76.882553] # Downtown Harrisburg, PA - Capital of Pennsylvania\n",
    "# Setup a search distance\n",
    "distance='25mi'\n",
    "# Set result type (can be 'recent', 'popular', or 'mixed')\n",
    "type_of_result='recent'\n",
    "# Set number of results (up to 100, remember you can only get 450 in 15 minutes)\n",
    "number_of_tweets=15\n",
    "\n",
    "\n",
    "# Fetches tweets with a given query at a given lat-long.\n",
    "def get_tweets_by_location( latlong=None ):\n",
    "    # Uses the search function to hit the APIs endpoints and look for recent tweets within the area\n",
    "    results = my_twython.search(q=search_term, geocode=str(latlong[0])+','+str(latlong[1])+','+ distance, result_type=type_of_result, count=number_of_tweets)\n",
    "    # Returns the only the statuses from the resulting JSON\n",
    "    return results['statuses']\n",
    "\n",
    "# test run our function\n",
    "get_tweets_by_location(latlong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Automate it - Hit the API and Parse the Result\n",
    "\n",
    "We are going to create a function to help us repeatedly hit the API, and parse the result into a readable JSON that contains the things that we are interested in, and still stores the raw tweet as an additional property. The returned object is a Python `dict` that we can easily parse into another dictionary to later store as a JSON. Raw JSONs returned from the API have a specific structure.\n",
    "\n",
    "It can be sometimes hard to read a raw JSON. I find it easy to use some online parsers like [this]( http://jsonparseronline.com/) to look at the structure of the JSON, and only access what we care about.\n",
    "\n",
    "Note: Remember we are limited to 450 every 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Does pretty much what its long name suggests.\n",
    "def get_lots_of_tweets( latlong ):\n",
    "    # Create a dictionary to parse the JSON\n",
    "    all_tweets = {}\n",
    "    \n",
    "    # We will be hitting the API a number of times within the total time\n",
    "    total_time = 10\n",
    "    \n",
    "    # Everytime we hit the API we subtract time from the total\n",
    "    remaining_seconds = total_time\n",
    "    interval = 5\n",
    "    while remaining_seconds > 0: # loop and run the function while remaining seconds is greater than zero\n",
    "        added = 0\n",
    "        # Hit the Twitter API using our function\n",
    "        new_tweets = get_tweets_by_location(latlong) # we set latlong above!\n",
    "        # Parse the resulting JSON, and save the rest of the raw content\n",
    "        for tweet in new_tweets:\n",
    "            tid = tweet['id']\n",
    "            if tid not in all_tweets:\n",
    "                properties = {}\n",
    "                if tweet['coordinates'] != None:\n",
    "                    properties['lat'] = tweet['coordinates']['coordinates'][0]\n",
    "                    properties['lon'] = tweet['coordinates']['coordinates'][1]\n",
    "                else:\n",
    "                    properties['lat'] = None\n",
    "                    properties['lon'] = None\n",
    "                properties['location'] = tweet['user']['location'] #This will get us the location associated with the profile\n",
    "                properties['tweet_id'] = tid\n",
    "                properties['content'] = tweet['text']\n",
    "                properties['user'] = tweet['user']['id']\n",
    "                properties['raw_source'] = tweet\n",
    "                properties['data_point'] = 'none'\n",
    "                properties['time'] = tweet['created_at']\n",
    "                all_tweets[ tid ] = properties\n",
    "                added += 1\n",
    "        print(\"At %d seconds, added %d new tweets, for a total of %d\" % ( total_time - remaining_seconds, added, len( all_tweets )))\n",
    "        # We wait a few seconds and hit the API again\n",
    "        time.sleep(interval)\n",
    "        remaining_seconds -= interval\n",
    "    print(str(len(all_tweets)) + ' Tweets retrieved.')\n",
    "    # We return the final dictionary to work with in Python\n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run the Twitter Scraper\n",
    "\n",
    "We need to call the functions, and save the JSONs into a location. In this case, I made a folder called **data**, where I am saving all the new JSONS. We can run the code continuously utilizing some loop, or we can use libraries like [threading](https://docs.python.org/3.6/library/threading.html).\n",
    "\n",
    "*Make sure you have a folder named **data** in the directory with your notebook! This function will save our collected datafiles to it when it finishes running.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function executes the the functions over a given period of time\n",
    "def run_all():\n",
    "    # This is the number of times the code will be executed. In this case, just once. \n",
    "    starting = 1\n",
    "    while starting > 0:\n",
    "        # Sometimes the API returns some errors, killing the whole script, so we setup try/except to make sure it keeps running\n",
    "        try:\n",
    "            t = get_lots_of_tweets( latlong )\n",
    "            # We name every file with the current time\n",
    "            timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "            # We write a new JSON into the target path\n",
    "            with open( 'data/' + '%stweets.json' %(timestr), 'w' ) as f:\n",
    "                f.write(json.dumps(t))\n",
    "            # we can use a library like threading to execute the run function continuously.\n",
    "            #threading.Timer(10, run).start()\n",
    "            starting -= 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Let's Explore the Data we Saved to our Machine\n",
    "\n",
    "Let's look through the JSON we created and checkout some of the data we just downloaded. First, let's import a couple of additional libraries that will let us interact with our file system, use numpy and pandas, and create plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import some additional libraries that will allow us to plot and interact with the operating system\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's explore our data a bit. This following function will allow us to perform a visual examination of what we downloaded from Twitter. Once we have collected some data, we can parse it, and visualize some of the results. Since some of the data is repeated, we can initialize some lists to check whether or not a tweet already exists, and add it to the list. We can then extract the useful information for our purposes, and store it in another list.\n",
    "\n",
    "Building on last week, we can do this in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the file names from a given directory\n",
    "file_dir = \"data\" # Set this to where your JSON saved\n",
    "# Get only the JSONs we have saved\n",
    "onlyfiles = [ f for f in listdir(file_dir) if isfile(join(file_dir,f)) and not f.startswith('.')]\n",
    "\n",
    "# create an empty dataframe with columns for each property\n",
    "df_tweets = pd.DataFrame(columns = ['tweet_id', 'lat', 'lon', 'content','location','user','raw_source','data_point','time'])\n",
    "\n",
    "# Loop through all the files\n",
    "for file in onlyfiles:\n",
    "    full_dir = join(file_dir,file) \n",
    "    with open(full_dir) as json_data:\n",
    "        dict = json.load(json_data) # creates a Python dictionary of our json\n",
    "        if not isinstance(dict, list):\n",
    "            for key, val in dict.items():\n",
    "                df_tweets.loc[key,val] = val\n",
    "                \n",
    "df_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Summarize, Group, and Plot the Dataset\n",
    "\n",
    "Next let's summarize and group our data, and create some plots.\n",
    "\n",
    "**A** - First, group by the user location. This is not lat/lon, but rather the information the individual has input on their profile.\n",
    "\n",
    "When you give people the power to input their own location, you get an assortment of 'creative' responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tweets.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tweets['location'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's do some grouping and sorting. We are using Pandas to do our analysis, much like last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_tweets = df_tweets.groupby('location')\n",
    "count_tweets = grouped_tweets['location'].count()\n",
    "df_count_tweets = count_tweets.to_frame()\n",
    "df_count_tweets.columns = ['Count']\n",
    "df_count_tweets.index.names = ['Location']\n",
    "df_count_tweets.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a pie chart of these using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list of colors (from iWantHue)\n",
    "colors = [\"#697dc6\",\"#5faf4c\",\"#7969de\",\"#b5b246\",\n",
    "          \"#cc54bc\",\"#4bad89\",\"#d84577\",\"#4eacd7\",\n",
    "          \"#cf4e33\",\"#894ea8\",\"#cf8c42\",\"#d58cc9\",\n",
    "          \"#737632\",\"#9f4b75\",\"#c36960\"]\n",
    "\n",
    "# Create a pie chart\n",
    "plt.pie( df_count_tweets['Count'], labels=df_count_tweets.index.get_values(), shadow=False, colors=colors)\n",
    "\n",
    "# View the plot drop above\n",
    "plt.axis('equal')\n",
    "\n",
    "# View the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few duplicates that we can clearly see, you will likely want to clean this data to rectify this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B** - Next, let's find how many actually have a location and put them on a scatterplot. To do this, we need to find first the rows where lat and long are not None, and then create the plot from just that subset of the data. Let's give it a go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a filter from df_tweets filtering only those that have values for lat and lon\n",
    "df_tweets_with_location = df_tweets[df_tweets.lon.notnull() & df_tweets.lat.notnull()]\n",
    "df_tweets_with_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use a scatter plot to make a quick visualization of the data points\n",
    "# NOTE: WHEN I DID THIS, I ONLY HAD SIX OUT OF ABOUT 100 TWEETS!\n",
    "plt.scatter(df_tweets_with_location['lon'],df_tweets_with_location['lat'], s=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Clean the Data\n",
    "\n",
    "**Clean the Locations** - You saw above that we had a bunch of locations that were very similar. Here, we could use [replace](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html). For example, we could replace all the variations of **York, PA** with a singular string using something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of variations of York\n",
    "variations_of_york = ['York, PA','York PA','York, Pennsylvania','York, Pa','41 W Market Street York, Pa','york P.a. ','York, Pa.']\n",
    "\n",
    "df_tweets['location'].replace(variations_of_york, 'York, PA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Duplicates** - Make sure that we don't get tweets plotted more than once. How would you make sure to only plot unique tweets? We can maybe use [drop_duplicates](http://chrisalbon.com/python/pandas_delete_duplicates.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Exporting your Data to CSV\n",
    "\n",
    "Exporting your dataset is easy, you can use **to_csv()**. This [Stack Exchange](http://stackoverflow.com/questions/16923281/pandas-writing-dataframe-to-csv-file) page will help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tweets.to_csv('twitter_data.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem Set 3 - Extend What You Have Learned\n",
    "\n",
    "Now that you know how to scrape data from Twitter, let's extend the exercise a little so you can show us what you know. This time you will set up the scraper to get data around MIT and scrape data for 20 minutes. Then you will visualize it with  and visualize. Think about what you would need to change to do that. \n",
    "\n",
    "Once you have the new JSON file of Boston tweets you should a pie chart and scatterplot of your collected tweets. When you are creating your dataset, you should get at least two different attributes returned by the Twitter API (we got many of them above, so base it off of that example). Atleast one of them should be the tweet id. Make sure you remove and duplicate tweets (if any). Expanding on the above, then save the data to a CSV.\n",
    "\n",
    "Make sure you get your own Twitter Key.\n",
    "\n",
    "#### Deliverables\n",
    "\n",
    "**1** - Using the Twitter REST API, collect Tweets from Boston for 30 min. Note how you set the time in the above example (in the **run_all()** function), it was in seconds. How would you do that here? \n",
    "\n",
    "**2** - Create a Pie Chart showing a summary of tweets by user location. Please clean up the data so that multiple variations of the same location name are replaced with one variation.\n",
    "\n",
    "**3** - Create a Scatterplot showing all of the tweets that had a latitude and longitude.\n",
    "\n",
    "**4** - Pick a search term, such as *Trump* or *#Trump* and collect 15 minutes of tweets on it. Use the same lat/lon for Boston as you used above.\n",
    "\n",
    "**5** - Export the entirety of your scraped Twitter datasets (one with a search term, one without) to two CSV files. We will be checking this CSV file for duplicates. So clean your file.  \n",
    "\n",
    "### What to Give Us on Stellar \n",
    "\n",
    "1 - Create a new Jupyter notebook that contains your scraper code. You can copy much of this one, but customize it. Submit the new Jupyter notebook, which includes your pie chart and scatterplot.\n",
    "\n",
    "2 - Your final CSV files. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
