{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anaconda Crash Course: Using Anaconda (“Conda”) to Supplement Python\n",
    "\n",
    "***\n",
    "\n",
    "Python is a valuable scripting language for data analysis and management; however managing a Python project environment can be nuanced and tricky. Anaconda is a platform built to complement Python by creating customizable and easily accessible environments in which you can run Python scripts. \n",
    "\n",
    "For reference, the Anaconda homepage is found at the following address.\n",
    "\n",
    "https://www.continuum.io/why-anaconda\n",
    "\n",
    "The following tutorial runs through the installation of Anaconda, and then introduces you to the concepts behind Anaconda that make it a nice and useful Python development environment.\n",
    "\n",
    "***\n",
    "\n",
    "### Install Anaconda (aka Conda)\n",
    "\n",
    "The Anaconda homepage contains the materials that you need to install Anaconda on your machine. You will primarily be using Anaconda through the command line, so you will have to get comfortable working on the command line. \n",
    "\n",
    "## 1. Check Anaconda Version and Install\n",
    "\n",
    "The first step is to open Terminal and check to see if you have Anaconda installed. If not, we will install it. To check the version, follow the following commands.\n",
    "\n",
    "#### i. Open Terminal\n",
    "#### ii. Check Version\n",
    "\n",
    "The syntax to access Anaconda on the command line is simply ‘conda’. To check the version you have installed, use the following:\n",
    "\n",
    "```sh\n",
    "\tconda info\n",
    "```\n",
    "\n",
    "If you have it installed, you will see platform information, version details, and environment paths after you hit enter, if not, the terminal will not recognize the command.\n",
    "\n",
    "\n",
    "#### iii. Install Anaconda\n",
    "\n",
    "To install ‘Conda’, navigate to the Anaconda downloads page at:\n",
    "\n",
    "[Anaconda Homepage and Downloads](https://www.continuum.io/downloads)\n",
    "\n",
    "Here, pick your system (Mac/Windows) and the Python version. In our case, we are going to pick Mac and select version 2.7. Use the graphical installer, it will provide us a wizard that will step us through the installation process.  Download the installer, double click the package file and follow the instructions.\n",
    "\n",
    "If you run into problems installing, or you get an error that states that Anaconda cannot be install in the default location, visit this page for short instructions on how to troubleshoot installation.\n",
    "\n",
    "[Anaconda Installation Docs](http://docs.continuum.io/anaconda/install#anaconda-install)\n",
    "\n",
    "Anaconda is contained in one directory, so if you ever need to uninstall Anaconda, use Terminal to remove the entire Anaconda directory using **rm -rf ~/anaconda**.\n",
    "\n",
    "We used Python 2.7 and not Python 3. The main reason for this is that Linux distributions and Mac still use Python 2.7 as default, and because the Python ecosystem has amassed a significant amount of quality software over the years in which some of it does not yet work on 3. Python 3, however, is designed to be more robust and fixes a lot of bugs in Python 2, so in the future, expect to see a continued migration to Python 3. For now, Python 2 will work just fine, and since we are using Anaconda, if we want to set up a Python 3 instance at some point, it will be easy to do!\n",
    "\n",
    "## 2. Confirm the installation worked properly\n",
    "\n",
    "Once we are finished with the installation, check to make sure it installed correctly by performing a version check.\n",
    "\n",
    "```sh\n",
    "\tconda info\n",
    "```\n",
    "\n",
    "If you see a 3.XX version number popup, and with platform and environment information, the installation worked. Now we can begin working with Conda.\n",
    "\n",
    "***\n",
    "\n",
    "## 3. The Anaconda 30-minute Test Drive\n",
    "\n",
    "Now let’s familiarize with what exactly Anaconda allows us to do. On a basic level, Anaconda is a Python distribution that adds many features and streamlines work with the language. It does this by creating specific environments on your machine in which you can specify the packages that are installed and used, and easily lets you toggle between environments. With in the individual environments, you can perform analysis, run scripts, and develop code.\n",
    "\n",
    "Environments are the bread and butter of Anaconda, because not all Python scripts you run will use the same packages, so you can customize exactly what you need, and create a sandbox that lets you try new things. Your environments will save the packages you have installed, allowing you to easily load an environment and run your scripts.\n",
    "\n",
    "The Anaconda team has put together a very nice Test Drive that is designed to take about a half hour that will introduce you to concepts around Anaconda, including setting up an environment, toggling between environments, managing the Python version you are using, managing the Python packages you are using in your environments, and finally, removing or uninstall packages and environments if you no longer need them.\n",
    "\n",
    "Follow the Test Drive at the following link:\n",
    "\n",
    "[Anaconda 30-minute Test Drive](http://conda.pydata.org/docs/test-drive.html)\n",
    "\n",
    "Working with Anaconda can make working with Python a much more pleasant experience. For additional resources, including cheatsheets and useful links, see the following materials.\n",
    "\n",
    "***\n",
    "\n",
    "## 4. Fire up Python - Web Scraping using Beautiful Soup\n",
    "\n",
    "Let's scrape some data. For the last part of this exercise, we are going to get into a bit of Python. Our goal will be to scrape some data a simple website to create a dataset of the best restaurants in Boston. Using a Python library called Beautiful Soup. To do so, we will create a Anaconda environment in which we can install modules and packages that we need to run the scraper (Beautiful Soup and Requests) To start, lets create an environment, like we just learned about in the Anaconda Test Drive.\n",
    "\n",
    "The website we are going to scrape is here.\n",
    "\n",
    "[Rat Incidents in Greater Boston](http://duspviz.mit.edu/_assets/data/sample.html)\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "\n",
    "#### i. Create Virtual Environment in Conda\n",
    "\n",
    "The first step is to create our virtual environment. In this environment, we can set up the packages and program versions we need to optimally run our script. Create an environment called **souperscraper** using the following syntax in terminal. We are going to install the Beautiful Soup program when we create the environment.\n",
    "\n",
    "```sh\n",
    "\tconda create --name souperscraper beautifulsoup4\n",
    "```\n",
    "\n",
    "Anaconda will ask install a handful of new packages that Python will work with in this environment, including Beautiful Soup version 4. When prompted, hit **y** to continue the installation. You should see that we are using Python 2.7, Beautiful Soup 4.4.1, and a handful of others.\n",
    "\n",
    "After creating the environment, launch the environment by typing the following. This will launch a virtual environment and allow us to access and run Python using the version and all packages we have installed into it.\n",
    "\n",
    "```sh\n",
    "\tsource activate souperscraper\n",
    "```\n",
    "\n",
    "All work we do now will be in our **souperscraper** environment.\n",
    "\n",
    "#### ii. Add Packages to Environment\n",
    "\n",
    "The next step is to a package we can use to access websites in our script. We have already installed beautiful soup, so the next package we want to install is called **requests**. Requests is a package that lets us make HTTP calls to websites from right inside our script. To install requests use **one** of the following syntaxes.\n",
    "\n",
    "Using [Easy Install](http://peak.telecommunity.com/DevCenter/EasyInstall) (a Python Packager, comes with Python/Conda)\n",
    "\n",
    "```sh\n",
    "\teasy_install requests\n",
    "```\n",
    "\n",
    "or [PIP](https://pip.pypa.io/en/stable/) (another Python Module Packager, perhaps more common, comes with Python/Conda)\n",
    "\n",
    "```sh\n",
    "\tpip install requests\n",
    "```\n",
    "\n",
    "Both of these methods will install requests. [Requests](http://docs.python-requests.org/en/latest/) is a library that makes it easy to access code from pages across the web, and view it in various forms. For data scraping, it is immensely useful.\n",
    "\n",
    "The workflow is the following: we will call a website using 'requests' and then parse it into components using 'beautifulsoup4'. Also, at this time, set your working directory in terminal to where you want files to save and update. To see your current working directory, type in **pwd** in terminal (this stands for Print Working Directory). To change, use **cd** followed by the path.\n",
    "\n",
    "#### iii. Simple Scraping Script using Beautiful Soup and Requests\n",
    "\n",
    "A great example to get into the usefulness of Python, especially for Big Data, is to try to get a bit of data. Open up your terminal to start working with Beautiful Soup on the command line. Start by firing up Python by entering the following.\n",
    "\n",
    "```sh\n",
    "\tpython\n",
    "```\n",
    "\n",
    "Python will launch in the terminal. Next, lets import modules. **import requests** imports the requests module, and **import bs4** imports the Beautiful Soup library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing out Requests\n",
    "\n",
    "Requests will allow us to load a webpage into python so that we can parse it and manipulate it. Test this by running the following. I am using a really simple page from the Beautiful Soup documentation to explain what is happening here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head><title>Where are the rats?</title></head>\n",
      "<body>\n",
      "<p class=\"title\"><b>Rat Incidents in Greater Boston</b></p>\n",
      "\n",
      "<p class=\"story\">The following is rodent incident data for \n",
      "<a href=\"http://example.com/boston\" class=\"link\" id=\"link1\">Boston</a>,\n",
      "<a href=\"http://example.com/brookline\" class=\"link\" id=\"link2\">Brookline</a>,\n",
      "<a href=\"http://example.com/cambridge\" class=\"link\" id=\"link2\">Cambridge</a>, and\n",
      "<a href=\"http://example.com/somerville\" class=\"link\" id=\"link3\">Somerville</a>;\n",
      "and it only available here.</p>\n",
      "\n",
      "<table>\n",
      "\t<thead>\n",
      "\t\t<tr>\n",
      "\t\t\t<th>City</th>\n",
      "\t\t\t<th># of rats</th>\n",
      "\t\t</tr>\n",
      "\t</thead>\n",
      "\t<tbody>\n",
      "\t\t<tr>\n",
      "\t\t\t<td class=\"city\">Cambridge</td>\n",
      "\t\t\t<td class=\"number\">400</td>\n",
      "\t\t</tr>\n",
      "\t\t<tr>\n",
      "\t\t\t<td class=\"city\">Boston</td>\n",
      "\t\t\t<td class=\"number\">900</td>\n",
      "\t\t</tr>\n",
      "\t\t<tr>\n",
      "\t\t\t<td class=\"city\">Somerville</td>\n",
      "\t\t\t<td class=\"number\">300</td>\n",
      "\t\t</tr>\n",
      "\t\t<tr>\n",
      "\t\t\t<td class=\"city\">Brookline</td>\n",
      "\t\t\t<td class=\"number\">600</td>\n",
      "\t\t</tr>\n",
      "\t</tbody>\n",
      "</table>\n",
      "\n",
      "</body>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('http://duspviz.mit.edu/_assets/data/sample.html')\n",
    "print response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allowed us to access all of the content from the source code of the webpage with Python, which we can now parse and extract data. Pretty cool!\n",
    "\n",
    "#### Testing out Beautiful Soup\n",
    "\n",
    "Our next big step is to test out Beautiful Soup. Let's talk about what this is...\n",
    "\n",
    "### What is Beautiful Soup?\n",
    "\n",
    "Beautiful Soup is a Python library for parsing data out of HTML and XML files (aka webpages). It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. The major concept with Beautiful Soup is that it allows you to access elements of your page by following the CSS structures, such as grabbing all links, all headers, specific classes, or more. It is a powerful library. Once we grab elements, Python makes it makes it easy to write the elements or relevant components of the elements into other files, such as a CSV, that can be stored in a database or opened in other software.\n",
    "\n",
    "The sample webpage we are using contains data on 'rodent incidents' in the greater Boston area. Let's use this file to explore the tree, and extract some data.\n",
    "\n",
    "#### iv. Make the Soup\n",
    "\n",
    "First, we have to turn the website code into a Python object. We have already imported the Beautiful Soup library, so we can start calling some of the methods in the libary. Replace **print response.text** with the following. This turns the text into an Python object named **soup**.\n",
    "\n",
    "An important note: You need to specify the specific parser that Beautiful Soup uses to parse your text. This is done in the second argument of the BeautifulSoup function. The default is the built in Python parser, which we can call using **html.parser**\n",
    "\n",
    "You an also use **lxml** or **html5lib**. This is nicely described in the [documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser). For our purposes, using the default is fine.\n",
    "\n",
    "Using the Beautiful Soup **prettify()** function, we can print the page to see the code printed in a readable and legible manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Where are the rats?\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    Rat Incidents in Greater Boston\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   The following is rodent incident data for\n",
      "   <a class=\"link\" href=\"http://example.com/boston\" id=\"link1\">\n",
      "    Boston\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"link\" href=\"http://example.com/brookline\" id=\"link2\">\n",
      "    Brookline\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"link\" href=\"http://example.com/cambridge\" id=\"link2\">\n",
      "    Cambridge\n",
      "   </a>\n",
      "   , and\n",
      "   <a class=\"link\" href=\"http://example.com/somerville\" id=\"link3\">\n",
      "    Somerville\n",
      "   </a>\n",
      "   ;\n",
      "and it only available here.\n",
      "  </p>\n",
      "  <table>\n",
      "   <thead>\n",
      "    <tr>\n",
      "     <th>\n",
      "      City\n",
      "     </th>\n",
      "     <th>\n",
      "      # of rats\n",
      "     </th>\n",
      "    </tr>\n",
      "   </thead>\n",
      "   <tbody>\n",
      "    <tr>\n",
      "     <td class=\"city\">\n",
      "      Cambridge\n",
      "     </td>\n",
      "     <td class=\"number\">\n",
      "      400\n",
      "     </td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "     <td class=\"city\">\n",
      "      Boston\n",
      "     </td>\n",
      "     <td class=\"number\">\n",
      "      900\n",
      "     </td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "     <td class=\"city\">\n",
      "      Somerville\n",
      "     </td>\n",
      "     <td class=\"number\">\n",
      "      300\n",
      "     </td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "     <td class=\"city\">\n",
      "      Brookline\n",
      "     </td>\n",
      "     <td class=\"number\">\n",
      "      600\n",
      "     </td>\n",
      "    </tr>\n",
      "   </tbody>\n",
      "  </table>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At any point, if you need a reference, visit the [Beautiful Soup documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) for the official descriptions of functions. Prettify is a handy one to see our document in a clean fashion.\n",
    "\n",
    "#### Navigating the Data Structure\n",
    "\n",
    "With our data from the webpage nicely laid out, Beautiful Soup allows us to now navigate the data structure. We called our Beautiful Soup object **soup**, so we can run the Beautiful Soup functions on this object. Let's explore some ways to do this, try entering some of the following into your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Where are the rats?</title>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the title element\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Where are the rats?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the content of the title element\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p class=\"title\"><b>Rat Incidents in Greater Boston</b></p>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access data in the first 'p' tag\n",
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"link\" href=\"http://example.com/boston\" id=\"link1\">Boston</a>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access data in the first 'a' tag\n",
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"link\" href=\"http://example.com/boston\" id=\"link1\">Boston</a>,\n",
       " <a class=\"link\" href=\"http://example.com/brookline\" id=\"link2\">Brookline</a>,\n",
       " <a class=\"link\" href=\"http://example.com/cambridge\" id=\"link2\">Cambridge</a>,\n",
       " <a class=\"link\" href=\"http://example.com/somerville\" id=\"link3\">Somerville</a>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve all links in the document (note it returns an array)\n",
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"link\" href=\"http://example.com/boston\" id=\"link1\">Boston</a>,\n",
       " <a class=\"link\" href=\"http://example.com/brookline\" id=\"link2\">Brookline</a>,\n",
       " <a class=\"link\" href=\"http://example.com/cambridge\" id=\"link2\">Cambridge</a>,\n",
       " <a class=\"link\" href=\"http://example.com/somerville\" id=\"link3\">Somerville</a>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve elements by class equal to link using the attributes argument\n",
    "soup.findAll(attrs={'class' : 'link'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"link\" href=\"http://example.com/somerville\" id=\"link3\">Somerville</a>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve a specific link by ID\n",
    "soup.find(id=\"link3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td class=\"city\">Cambridge</td>,\n",
       " <td class=\"number\">400</td>,\n",
       " <td class=\"city\">Boston</td>,\n",
       " <td class=\"number\">900</td>,\n",
       " <td class=\"city\">Somerville</td>,\n",
       " <td class=\"number\">300</td>,\n",
       " <td class=\"city\">Brookline</td>,\n",
       " <td class=\"number\">600</td>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access Data in the table (note it returns an array)\n",
    "soup.find_all('td')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Arrays\n",
    "\n",
    "The easiest way to access elements and then either write them to file or manipulate them is to save them as objects themselves. Note that our data is organzed into cities and numbers. Let's save these to arrays, which are the easiest way to work with the data.\n",
    "\n",
    "The following gives us an array, we can work with the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cambridge\n",
      "Boston\n",
      "Somerville\n",
      "Brookline\n"
     ]
    }
   ],
   "source": [
    "data = soup.findAll(attrs={'class':'city'})\n",
    "print data[0].string\n",
    "print data[1].string\n",
    "print data[2].string\n",
    "print data[3].string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You never want to repeat code like this, so turn this into a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cambridge\n",
      "Boston\n",
      "Somerville\n",
      "Brookline\n"
     ]
    }
   ],
   "source": [
    "data = soup.findAll(attrs={'class':'city'})\n",
    "for i in data:\n",
    "    print i.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array only gives us cities though, let's get all of the data elements that have either class **city** or class **number**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<td class=\"city\">Cambridge</td>, <td class=\"number\">400</td>, <td class=\"city\">Boston</td>, <td class=\"number\">900</td>, <td class=\"city\">Somerville</td>, <td class=\"number\">300</td>, <td class=\"city\">Brookline</td>, <td class=\"number\">600</td>]\n"
     ]
    }
   ],
   "source": [
    "data = soup.findAll(attrs={'class':['city','number']})\n",
    "print data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all of our data that was nested in these tags saved to a Python array. Access the elements of the array by using data[x], where x is location in the array. In Python, arrays start at 0, so place 1 in a Python array is actually called by using a 0, and place 8 would be called by a 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td class=\"city\">Cambridge</td>\n",
      "<td class=\"number\">400</td>\n"
     ]
    }
   ],
   "source": [
    "print data[0]\n",
    "print data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, we get the whole element with those commands. To get just the content, use the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cambridge\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "print data[0].string\n",
    "print data[1].string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Data to a File using a Simple Loop\n",
    "\n",
    "Python makes opening a file and writing to it very easy. Let's take this simple dataset and write it to a file that saves in our current working directory. An important note, whatever the working directory is when you start Python will be the root for where your files are read from and written to.\n",
    "\n",
    "Python also has nice iteration features that allow us to iterate through arrays, lists, and other files. In this following example, manually create a comma-separated document with our data using file writing operations and a while loop.\n",
    "\n",
    "In pseudo-code:\n",
    "\n",
    "1. Open up a file to write in and append data.\n",
    "\n",
    "2. Set up parameters for the while loop\n",
    "\n",
    "3. Write headers\n",
    "\n",
    "4. Run while loop that will write elements of the array to file\n",
    "\n",
    "5. When complete, close the file\n",
    "\n",
    "Once done, open the file on your machine and see your data. Enter the following code, note what each line is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('rat_data.txt','a') # open new file\n",
    "\n",
    "p = 0 # initial place in array\n",
    "l = len(data)-1 # length of array minus one\n",
    "\n",
    "f.write(\"City, Number\\n\") #write headers\n",
    "\n",
    "while p < l: # while place is less than length\n",
    "    f.write(data[p].string + \", \") # write city and add comma\n",
    "    p = p + 1 # increment\n",
    "    f.write(data[p].string + \"\\n\") # write number and line break\n",
    "    p = p + 1 # increment\n",
    "\n",
    "f.close() # close file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Open **rat_data.txt** on your machine. It's a CSV with the data from the page!\n",
    "\n",
    "---\n",
    "\n",
    "```sh\n",
    "City, Number\n",
    "Cambridge, 400\n",
    "Boston, 900\n",
    "Somerville, 300\n",
    "Brookline, 600\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "To see a completed script to extract this data from the rat data website, view **rat_data.txt** in the materials.\n",
    "\n",
    "Further reading on File Reading and Writing and Iteration in Python can be found at the following links.\n",
    "\n",
    "[Python File Reading and Writing Documentation](https://docs.python.org/2/tutorial/inputoutput.html#reading-and-writing-files)\n",
    "\n",
    "[Tutorials Point Python While Loop Statements](http://www.tutorialspoint.com/python/python_while_loop.htm)\n",
    "\n",
    "We created this CSV manually to illustrate some basic Python. Python has modules and features that support CSV as well that are very useful and handy. These are best if you are reading in a CSV, allowing you to access the elements of the CSV. You can read more about the built in CSV module [here](https://docs.python.org/2/library/csv.html).\n",
    "\n",
    "You've got your feet wet, over the next weeks, there will be much more to come on Python and data scraping!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "\n",
    "## Additional Reading and Resources\n",
    "\n",
    "#### Conda Command Line Cheatsheet -\n",
    "http://conda.pydata.org/docs/_downloads/conda-cheatsheet.pdf\n",
    "\n",
    "#### Mac Command Line Cheatsheet –\n",
    "https://github.com/0nn0/terminal-mac-cheatsheet/wiki/Terminal-Cheatsheet-for-Mac-(-basics-)\n",
    "\n",
    "#### Python Documentation -\n",
    "https://docs.python.org/2/library/index.html\n",
    "\n",
    "#### Beautiful Soup Tutorials -\n",
    "http://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
