{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anaconda Crash Course: Using Anaconda (“Conda”) to Supplement Python\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "## 4. Fire up Python - Web Scraping using Beautiful Soup\n",
    "\n",
    "Let's scrape some data. For the last part of this exercise, we are going to get into a bit of Python. Our goal will be to scrape some data a simple website to create a dataset of the best restaurants in Boston. Using a Python library called Beautiful Soup. To do so, we will create a Anaconda environment in which we can install modules and packages that we need to run the scraper (Beautiful Soup and Requests) To start, lets create an environment, like we just learned about in the Anaconda Test Drive.\n",
    "\n",
    "The website we are going to scrape is here.\n",
    "\n",
    "[Rat Incidents in Greater Boston](http://duspviz.mit.edu/_assets/data/sample.html)\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "\n",
    "#### i. Create Virtual Environment in Conda\n",
    "\n",
    "The first step is to create our virtual environment. In this environment, we can set up the packages and program versions we need to optimally run our script. Create an environment called **souperscraper** using the following syntax in terminal. We are going to install the Beautiful Soup program when we create the environment.\n",
    "\n",
    "```sh\n",
    "\tconda create --name souperscraper beautifulsoup4\n",
    "```\n",
    "\n",
    "Anaconda will ask install a handful of new packages that Python will work with in this environment, including Beautiful Soup version 4. When prompted, hit **y** to continue the installation. You should see that we are using Python 2.7, Beautiful Soup 4.4.1, and a handful of others.\n",
    "\n",
    "After creating the environment, launch the environment by typing the following. This will launch a virtual environment and allow us to access and run Python using the version and all packages we have installed into it.\n",
    "\n",
    "```sh\n",
    "\tsource activate souperscraper\n",
    "```\n",
    "\n",
    "All work we do now will be in our **souperscraper** environment.\n",
    "\n",
    "#### ii. Add Packages to Environment\n",
    "\n",
    "The next step is to a package we can use to access websites in our script. We have already installed beautiful soup, so the next package we want to install is called **requests**. Requests is a package that lets us make HTTP calls to websites from right inside our script. To install requests use **one** of the following syntaxes.\n",
    "\n",
    "Using [Easy Install](http://peak.telecommunity.com/DevCenter/EasyInstall) (a Python Packager, comes with Python/Conda)\n",
    "\n",
    "```sh\n",
    "\teasy_install requests\n",
    "```\n",
    "\n",
    "or [PIP](https://pip.pypa.io/en/stable/) (another Python Module Packager, perhaps more common, comes with Python/Conda)\n",
    "\n",
    "```sh\n",
    "\tpip install requests\n",
    "```\n",
    "\n",
    "Both of these methods will install requests. [Requests](http://docs.python-requests.org/en/latest/) is a library that makes it easy to access code from pages across the web, and view it in various forms. For data scraping, it is immensely useful.\n",
    "\n",
    "The workflow is the following: we will call a website using 'requests' and then parse it into components using 'beautifulsoup4'. Also, at this time, set your working directory in terminal to where you want files to save and update. To see your current working directory, type in **pwd** in terminal (this stands for Print Working Directory). To change, use **cd** followed by the path.\n",
    "\n",
    "#### iii. Simple Scraping Script using Beautiful Soup and Requests\n",
    "\n",
    "A great example to get into the usefulness of Python, especially for Big Data, is to try to get a bit of data. Open up your terminal to start working with Beautiful Soup on the command line. Start by firing up Python by entering the following.\n",
    "\n",
    "```sh\n",
    "\tpython\n",
    "```\n",
    "\n",
    "Python will launch in the terminal. Next, lets import modules. **import requests** imports the requests module, and **import bs4** imports the Beautiful Soup library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing out Requests\n",
    "\n",
    "Requests will allow us to load a webpage into python so that we can parse it and manipulate it. Test this by running the following. I am using a really simple page from the Beautiful Soup documentation to explain what is happening here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head><title>Where are the rats?</title></head>\n",
      "<body>\n",
      "<p class=\"title\"><b>Rat Incidents in Greater Boston</b></p>\n",
      "\n",
      "<p class=\"story\">The following is rodent incident data for \n",
      "<a href=\"http://example.com/boston\" class=\"link\" id=\"link1\">Boston</a>,\n",
      "<a href=\"http://example.com/brookline\" class=\"link\" id=\"link2\">Brookline</a>,\n",
      "<a href=\"http://example.com/cambridge\" class=\"link\" id=\"link2\">Cambridge</a>, and\n",
      "<a href=\"http://example.com/somerville\" class=\"link\" id=\"link3\">Somerville</a>;\n",
      "and it only available here.</p>\n",
      "\n",
      "<table>\n",
      "\t<thead>\n",
      "\t\t<tr>\n",
      "\t\t\t<th>City</th>\n",
      "\t\t\t<th># of rats</th>\n",
      "\t\t</tr>\n",
      "\t</thead>\n",
      "\t<tbody>\n",
      "\t\t<tr>\n",
      "\t\t\t<td class=\"city\">Cambridge</td>\n",
      "\t\t\t<td class=\"number\">400</td>\n",
      "\t\t</tr>\n",
      "\t\t<tr>\n",
      "\t\t\t<td class=\"city\">Boston</td>\n",
      "\t\t\t<td class=\"number\">900</td>\n",
      "\t\t</tr>\n",
      "\t\t<tr>\n",
      "\t\t\t<td class=\"city\">Somerville</td>\n",
      "\t\t\t<td class=\"number\">300</td>\n",
      "\t\t</tr>\n",
      "\t\t<tr>\n",
      "\t\t\t<td class=\"city\">Brookline</td>\n",
      "\t\t\t<td class=\"number\">600</td>\n",
      "\t\t</tr>\n",
      "\t</tbody>\n",
      "</table>\n",
      "\n",
      "</body>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = requests.get('http://duspviz.mit.edu/_assets/data/sample.html')\n",
    "print response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allowed us to access all of the content from the source code of the webpage with Python, which we can now parse and extract data. Pretty cool!\n",
    "\n",
    "#### Testing out Beautiful Soup\n",
    "\n",
    "Our next big step is to test out Beautiful Soup. Let's talk about what this is...\n",
    "\n",
    "### What is Beautiful Soup?\n",
    "\n",
    "Beautiful Soup is a Python library for parsing data out of HTML and XML files (aka webpages). It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. The major concept with Beautiful Soup is that it allows you to access elements of your page by following the CSS structures, such as grabbing all links, all headers, specific classes, or more. It is a powerful library. Once we grab elements, Python makes it makes it easy to write the elements or relevant components of the elements into other files, such as a CSV, that can be stored in a database or opened in other software.\n",
    "\n",
    "The sample webpage we are using contains data on 'rodent incidents' in the greater Boston area. Let's use this file to explore the tree, and extract some data.\n",
    "\n",
    "#### iv. Make the Soup\n",
    "\n",
    "First, we have to turn the website code into a Python object. We have already imported the Beautiful Soup library, so we can start calling some of the methods in the libary. Replace **print response.text** with the following. This turns the text into an Python object named **soup**.\n",
    "\n",
    "An important note: You need to specify the specific parser that Beautiful Soup uses to parse your text. This is done in the second argument of the BeautifulSoup function. The default is the built in Python parser, which we can call using **html.parser**\n",
    "\n",
    "You an also use **lxml** or **html5lib**. This is nicely described in the [documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser). For our purposes, using the default is fine.\n",
    "\n",
    "Using the Beautiful Soup **prettify()** function, we can print the page to see the code printed in a readable and legible manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Where are the rats?\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    Rat Incidents in Greater Boston\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   The following is rodent incident data for\n",
      "   <a class=\"link\" href=\"http://example.com/boston\" id=\"link1\">\n",
      "    Boston\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"link\" href=\"http://example.com/brookline\" id=\"link2\">\n",
      "    Brookline\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"link\" href=\"http://example.com/cambridge\" id=\"link2\">\n",
      "    Cambridge\n",
      "   </a>\n",
      "   , and\n",
      "   <a class=\"link\" href=\"http://example.com/somerville\" id=\"link3\">\n",
      "    Somerville\n",
      "   </a>\n",
      "   ;\n",
      "and it only available here.\n",
      "  </p>\n",
      "  <table>\n",
      "   <thead>\n",
      "    <tr>\n",
      "     <th>\n",
      "      City\n",
      "     </th>\n",
      "     <th>\n",
      "      # of rats\n",
      "     </th>\n",
      "    </tr>\n",
      "   </thead>\n",
      "   <tbody>\n",
      "    <tr>\n",
      "     <td class=\"city\">\n",
      "      Cambridge\n",
      "     </td>\n",
      "     <td class=\"number\">\n",
      "      400\n",
      "     </td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "     <td class=\"city\">\n",
      "      Boston\n",
      "     </td>\n",
      "     <td class=\"number\">\n",
      "      900\n",
      "     </td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "     <td class=\"city\">\n",
      "      Somerville\n",
      "     </td>\n",
      "     <td class=\"number\">\n",
      "      300\n",
      "     </td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "     <td class=\"city\">\n",
      "      Brookline\n",
      "     </td>\n",
      "     <td class=\"number\">\n",
      "      600\n",
      "     </td>\n",
      "    </tr>\n",
      "   </tbody>\n",
      "  </table>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At any point, if you need a reference, visit the [Beautiful Soup documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) for the official descriptions of functions. Prettify is a handy one to see our document in a clean fashion.\n",
    "\n",
    "#### Navigating the Data Structure\n",
    "\n",
    "With our data from the webpage nicely laid out, Beautiful Soup allows us to now navigate the data structure. We called our Beautiful Soup object **soup**, so we can run the Beautiful Soup functions on this object. Let's explore some ways to do this, try entering some of the following into your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Where are the rats?</title>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the title element\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Where are the rats?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the content of the title element\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p class=\"title\"><b>Rat Incidents in Greater Boston</b></p>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access data in the first 'p' tag\n",
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"link\" href=\"http://example.com/boston\" id=\"link1\">Boston</a>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access data in the first 'a' tag\n",
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"link\" href=\"http://example.com/boston\" id=\"link1\">Boston</a>,\n",
       " <a class=\"link\" href=\"http://example.com/brookline\" id=\"link2\">Brookline</a>,\n",
       " <a class=\"link\" href=\"http://example.com/cambridge\" id=\"link2\">Cambridge</a>,\n",
       " <a class=\"link\" href=\"http://example.com/somerville\" id=\"link3\">Somerville</a>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve all links in the document (note it returns an array)\n",
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"link\" href=\"http://example.com/boston\" id=\"link1\">Boston</a>,\n",
       " <a class=\"link\" href=\"http://example.com/brookline\" id=\"link2\">Brookline</a>,\n",
       " <a class=\"link\" href=\"http://example.com/cambridge\" id=\"link2\">Cambridge</a>,\n",
       " <a class=\"link\" href=\"http://example.com/somerville\" id=\"link3\">Somerville</a>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve elements by class equal to link using the attributes argument\n",
    "soup.findAll(attrs={'class' : 'link'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"link\" href=\"http://example.com/somerville\" id=\"link3\">Somerville</a>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve a specific link by ID\n",
    "soup.find(id=\"link3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td class=\"city\">Cambridge</td>,\n",
       " <td class=\"number\">400</td>,\n",
       " <td class=\"city\">Boston</td>,\n",
       " <td class=\"number\">900</td>,\n",
       " <td class=\"city\">Somerville</td>,\n",
       " <td class=\"number\">300</td>,\n",
       " <td class=\"city\">Brookline</td>,\n",
       " <td class=\"number\">600</td>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access Data in the table (note it returns an array)\n",
    "soup.find_all('td')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Arrays\n",
    "\n",
    "The easiest way to access elements and then either write them to file or manipulate them is to save them as objects themselves. Note that our data is organzed into cities and numbers. Let's save these to arrays, which are the easiest way to work with the data.\n",
    "\n",
    "The following gives us an array, we can work with the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cambridge\n",
      "Boston\n",
      "Somerville\n",
      "Brookline\n"
     ]
    }
   ],
   "source": [
    "data = soup.findAll(attrs={'class':'city'})\n",
    "print data[0].string\n",
    "print data[1].string\n",
    "print data[2].string\n",
    "print data[3].string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You never want to repeat code like this, so turn this into a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cambridge\n",
      "Boston\n",
      "Somerville\n",
      "Brookline\n"
     ]
    }
   ],
   "source": [
    "data = soup.findAll(attrs={'class':'city'})\n",
    "for i in data:\n",
    "    print i.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array only gives us cities though, let's get all of the data elements that have either class **city** or class **number**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<td class=\"city\">Cambridge</td>, <td class=\"number\">400</td>, <td class=\"city\">Boston</td>, <td class=\"number\">900</td>, <td class=\"city\">Somerville</td>, <td class=\"number\">300</td>, <td class=\"city\">Brookline</td>, <td class=\"number\">600</td>]\n"
     ]
    }
   ],
   "source": [
    "data = soup.findAll(attrs={'class':['city','number']})\n",
    "print data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all of our data that was nested in these tags saved to a Python array. Access the elements of the array by using data[x], where x is location in the array. In Python, arrays start at 0, so place 1 in a Python array is actually called by using a 0, and place 8 would be called by a 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td class=\"city\">Cambridge</td>\n",
      "<td class=\"number\">400</td>\n"
     ]
    }
   ],
   "source": [
    "print data[0]\n",
    "print data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, we get the whole element with those commands. To get just the content, use the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cambridge\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "print data[0].string\n",
    "print data[1].string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Data to a File using a Simple Loop\n",
    "\n",
    "Python makes opening a file and writing to it very easy. Let's take this simple dataset and write it to a file that saves in our current working directory. An important note, whatever the working directory is when you start Python will be the root for where your files are read from and written to.\n",
    "\n",
    "Python also has nice iteration features that allow us to iterate through arrays, lists, and other files. In this following example, manually create a comma-separated document with our data using file writing operations and a while loop.\n",
    "\n",
    "In pseudo-code:\n",
    "\n",
    "1. Open up a file to write in and append data.\n",
    "\n",
    "2. Set up parameters for the while loop\n",
    "\n",
    "3. Write headers\n",
    "\n",
    "4. Run while loop that will write elements of the array to file\n",
    "\n",
    "5. When complete, close the file\n",
    "\n",
    "Once done, open the file on your machine and see your data. Enter the following code, note what each line is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('rat_data.txt','a') # open new file\n",
    "\n",
    "p = 0 # initial place in array\n",
    "l = len(data)-1 # length of array minus one\n",
    "\n",
    "f.write(\"City, Number\\n\") #write headers\n",
    "\n",
    "while p < l: # while place is less than length\n",
    "    f.write(data[p].string + \", \") # write city and add comma\n",
    "    p = p + 1 # increment\n",
    "    f.write(data[p].string + \"\\n\") # write number and line break\n",
    "    p = p + 1 # increment\n",
    "\n",
    "f.close() # close file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Open **rat_data.txt** on your machine. It's a CSV with the data from the page!\n",
    "\n",
    "---\n",
    "\n",
    "```sh\n",
    "City, Number\n",
    "Cambridge, 400\n",
    "Boston, 900\n",
    "Somerville, 300\n",
    "Brookline, 600\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "To see a completed script to extract this data from the rat data website, view **rat_data.txt** in the materials.\n",
    "\n",
    "Further reading on File Reading and Writing and Iteration in Python can be found at the following links.\n",
    "\n",
    "[Python File Reading and Writing Documentation](https://docs.python.org/2/tutorial/inputoutput.html#reading-and-writing-files)\n",
    "\n",
    "[Tutorials Point Python While Loop Statements](http://www.tutorialspoint.com/python/python_while_loop.htm)\n",
    "\n",
    "We created this CSV manually to illustrate some basic Python. Python has modules and features that support CSV as well that are very useful and handy. These are best if you are reading in a CSV, allowing you to access the elements of the CSV. You can read more about the built in CSV module [here](https://docs.python.org/2/library/csv.html).\n",
    "\n",
    "You've got your feet wet, over the next weeks, there will be much more to come on Python and data scraping!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "\n",
    "## Additional Reading and Resources\n",
    "\n",
    "#### Conda Command Line Cheatsheet -\n",
    "http://conda.pydata.org/docs/_downloads/conda-cheatsheet.pdf\n",
    "\n",
    "#### Mac Command Line Cheatsheet –\n",
    "https://github.com/0nn0/terminal-mac-cheatsheet/wiki/Terminal-Cheatsheet-for-Mac-(-basics-)\n",
    "\n",
    "#### Python Documentation -\n",
    "https://docs.python.org/2/library/index.html\n",
    "\n",
    "#### Beautiful Soup Tutorials -\n",
    "http://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
